---
title: "subset_modeling"
output: html_document
---

In this portion of the analysis, we will perform modeling on a smaller subset of data in attempt to improve model performance. The original aim of this document was to perform feature selection/subsetting for model improvement. However, the implementation of the LASSO method eliminated the need for this by essentially automating subset selection. The contents of this document are *not* needed for the final analysis/manuscript product, but are kept in the project repository for understanding of the whole analysis process. 

# load packages and data
```{r}
library(ggplot2) #for plotting
library(broom) #for cleaning up output from lm()
library(here) #for data loading/saving
library(tidymodels) #for modeling
library(rpart)
library(glmnet)
library(ranger)
library(rpart.plot)  # for visualizing a decision tree
library(vip)         # for variable importance plots

#path to data

#load cleaned USBSET data. 
data <- readRDS(here("data", "processed_data", "data_subset.rds"))
```

# split data into train and test subsets
```{r}
# set seed for reproducible analysis (instead of random subset each time)
set.seed(123)
#subset 3/4 of data as training set
data_split <- initial_split(data, 
                            prop = 7/10, 
                            strata = particles_l) #stratify by MP concentration for balanced outcome

#save sets as data frames
train_data <- training(data_split)
test_data <- testing(data_split)
```

# Cross validation
We want to perform 5-fold CV, 5 times repeated
```{r}
#create folds (resample object)
set.seed(123)
folds <- vfold_cv(train_data, 
                  v = 5, 
                  repeats = 5,
                  strata = particles_l) #folds is set up to perform our CV

#linear model set up
lm_mod <- linear_reg() %>% 
            set_engine('lm') %>% 
            set_mode('regression')

#create recipe for data and fitting and make dummy variables
MP_rec <- recipe(particles_l ~ ., data = train_data) %>% step_dummy(all_nominal())

#workflow set up
MP_wflow <- 
  workflow() %>% add_model(lm_mod) %>% add_recipe(MP_rec)

#use workflow to prepare recipe and train model with predictors
MP_fit <- 
  MP_wflow %>% fit(data = train_data)

#extract model coefficient
MP_fit %>% extract_fit_parsnip() %>% tidy()
```

# Null model performance
```{r}
#recipe for null model
null_train_rec <- recipe(particles_l ~ 1, data = train_data) #predicts mean of outcome

#null model workflow incorporating null model recipe
null_wflow <- workflow() %>% add_model(lm_mod) %>% add_recipe(null_train_rec)

# I want to check and make sure that the null model worked as it was supposed to, so I want to view the predictions and make sure they are all the mean of the outcome
#get fit for train data using null workflow
nullfittest <- null_wflow %>% fit(data = train_data)
#get predictions based on null model
prediction <- predict(nullfittest, train_data)
test_pred <- predict(nullfittest, test_data)
#the predictions for the train and test data are all the same mean value, so this tells us the null model was set up properly

#Now, we'll use fit_resamples based on the tidymodels tutorial for CV/resampling (https://www.tidymodels.org/start/resampling/)
#fit model with training data
null_fit_train <- fit_resamples(null_wflow, resamples = folds)

#get results
metrics_null_train <- collect_metrics(null_fit_train)
#RMSE for null train fit is 109.4

#repeat for test data
null_test_rec <- recipe(particles_l ~ 1, data = test_data) #predicts mean of outcome
null_test_wflow <- workflow() %>% add_model(lm_mod) %>% add_recipe(null_test_rec) #sets workflow with new test recipe
null_fit_test <- fit_resamples(null_test_wflow, resamples = folds) #performs fit
metrics_null_test <- collect_metrics(null_fit_test) #gets fit metrics
#RMSE for null test fit is 109.4
```

# Model tuning and fitting
Include:

1. Model specification
2. Workflow definition
3. Tuning grid specification
4. Tuning w/ cross-validation + `tune_grid()`

## LASSO model
```{r}

#cross validation
set.seed(123)
cell_folds <- vfold_cv(train_data)

#model specification
#lasso <- linear_reg(penalty = tune()) %>% set_engine("glmnet") %>% set_mode("regression")
lasso <- linear_reg() %>%
  set_mode("regression") %>%           
  set_engine("glmnet") %>%
  set_args(penalty = tune(), mixture = 1)
#set workflow
lasso_wf <- workflow() %>% add_model(lasso) %>% add_recipe(MP_rec)

#tuning grid specification
lasso_grid <- tibble(penalty = 10^seq(-3, 0, length.out = 30))

#tuning with CV and tune_grid
lasso_res <- lasso_wf %>% tune_grid(resamples = cell_folds, 
                                    grid = lasso_grid, 
                                    control = control_grid(save_pred = TRUE), 
                                    metrics = metric_set(rmse))
#view model metrics
lasso_res %>% collect_metrics()

#select top models
top_lasso <- 
  lasso_res %>% show_best("rmse") %>% arrange(penalty)
top_lasso #view

#see best lasso
best_lasso <- lasso_res %>% select_best()
best_lasso #view

#finalize workflow with top model
lasso_final_wf <- lasso_wf %>% finalize_workflow(best_lasso)

#fit model with finalized WF
lasso_fit <- lasso_final_wf %>% fit(train_data)
```

### LASSO plots
```{r}
#diagnostics
autoplot(lasso_res)
#calculate residuals
lasso_resid <- lasso_fit %>%
  augment(train_data) %>% #this will add predictions to our df
  select(.pred, particles_l) %>%
  mutate(.resid = particles_l - .pred) #manually calculate residuals

#model predictions from tuned model vs actual outcomes
lasso_pred_plot <- ggplot(lasso_resid, aes(x = particles_l, y = .pred)) + geom_point() + 
  labs(title = "Predictions vs Actual Outcomes: LASSO", x = "Particles/L Outcome", y = "Particles/L Prediction")
lasso_pred_plot


#plot residuals vs predictions
lasso_resid_plot <- ggplot(lasso_resid, aes(y = .resid, x = .pred)) + geom_point() + 
  labs(title = "Predictions vs Residuals: LASSO", x = "Particles/L Prediction", y = "Residuals")
lasso_resid_plot #view plot

#compare to null model
metrics_null_train #view null RMSE for train data
lasso_res %>% show_best(n=1) #view RMSE for best lasso model

```

Next, I want to see what predictors are used in the best lasso model. also need to compare to null model

RMSE for best LASSO model on the subset data is 113.09, compared to 109.4 RMSE for the null model. This is definitely not a well-performing model, since we want a better RMSE for any model we build than the RMSE for the null model. 

In the perfect outcome/prediction plot, you hope to see data scattered along a 45 degree diagonal line. Instead, data is clustered. However, the scales of the x and y axis are different which may impact how we are viewing the data. Let's visualize with a better scale.

```{r}
#model predictions from tuned model vs actual outcomes SCALE ADJUSTED
lasso_pred_plot_adj <- ggplot(lasso_resid, aes(x = particles_l, y = .pred)) + geom_point() + 
  labs(title = "Predictions vs Actual Outcomes: LASSO", x = "Particles/L Outcome", y = "Particles/L Prediction") + ylim(0, 1000) + xlim(0,1000)
lasso_pred_plot_adj
```

Now that we have adjusted the scale, our outcome vs prediction plot looks decent, and there don't appear to be systematic deviations. This indicates that our LASSO model may be flexible enough for our data. 

#plot for how the number of predictors included in the LASSO model changes with the tuning parameter
```{r}
x <- lasso_fit$fit$fit$fit 
plot(x, "lambda")

summary(best_lasso)

coefficients <- lasso_fit %>% extract_fit_parsnip() %>% tidy()
lasso_fit %>% extract_fit_parsnip() %>% tidy()

```

Try modeling with data subset, including these variables: "particles_l", "visual_score", "turbidity.ntu", "temperature.c", "e.coli.cfu", "population", "dist".


The results we get from the initial machine learning modeling and from the modeling of this subset data are not great. Let's think of some ways that we can go about improving our model. 

- An option is variable/predictor removal. We tried that with the subset data. We could attempt further subsets going forward. 
- Variable transformation. Some microplastic studies use log transformation on the concentration values to get a normal distribution. 
- However, regularization may accomplish our goals more efficiently - we are advised that subset selection is not always a great idea and doesn't neccesarily do the best job of addressing overfitting
- Could try ridge regression for alternate form of regularization 
- Could try decision tree (though we learned these usually don't perform as well as LASSO)

# Model selection : LASSO
```{r}
#We've selected the LASSO model. Let's evaluate it with the test data. 

#fit to test data
last_lasso_fit <- lasso_final_wf %>% last_fit(data_split)
last_lasso_fit %>% collect_metrics()
```

It appears that the lasso model receives an RMSE of ~197 when run with the test data. Compared to the RMSE of 113 with the train data, this is not great performance and indicates that the model overfits the train data. 

# Variable importance
```{r}
library(parsnip)
last_lasso_fit %>% 
  purrr::pluck(".workflow", 1) %>%   
  workflows::extract_fit_parsnip() %>% 
  vip(num_features = 20)

```

This plot shows us the variable importance rankings for the LASSO model, and we see that the watershed identity of each each sample site has importance for predicting microplastics concentration, while the hypothesized predictors (population, distance from WWTP, turbidity, e.coli.cfu, visual score) do not have importance in this model. 




```